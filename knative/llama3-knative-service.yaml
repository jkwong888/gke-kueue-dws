apiVersion: serving.knative.dev/v1
kind: Service
metadata:
  name: llama3
spec:
  template:
    metadata:
      labels:
        kueue.x-k8s.io/queue-name: a100-80gb
    spec:
      nodeSelector:
        cloud.google.com/gke-nodepool: kueue-dev-a100-80gb-nodepool
      tolerations:
      - key: "nvidia.com/gpu"
        operator: "Exists"
        effect: "NoSchedule"
      - key: "kueue"
        operator: "Equal"
        value: "true"
        effect: "NoSchedule"
      containers:
        - name: vllm
          command:
          - python3
          - -m
          - vllm.entrypoints.openai.api_server
          - --model
          - meta-llama/Meta-Llama-3.1-8B-Instruct
          image: vllm/vllm-openai:latest
          livenessProbe:
            failureThreshold: 3
            httpGet:
              path: /health
              port: 8000
              scheme: HTTP
            initialDelaySeconds: 240
            periodSeconds: 5
            successThreshold: 1
            timeoutSeconds: 1
          env:
            - name: HUGGING_FACE_HUB_TOKEN
              valueFrom:
                secretKeyRef:
                  name: hf-token
                  key: token
          ports:
            - containerPort: 8000
              protocol: TCP
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /health
              port: 8000
              scheme: HTTP
            initialDelaySeconds: 240
            periodSeconds: 5
            successThreshold: 1
            timeoutSeconds: 1
          resources:
            requests:
              nvidia.com/gpu: "1"
            limits:
              nvidia.com/gpu: "1"
          volumeMounts:
          - mountPath: /root/.cache/huggingface
            name: cache-volume
      volumes:
      - emptyDir: {}
        name: cache-volume