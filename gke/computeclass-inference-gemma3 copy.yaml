# define a compute class for gemma3 inference workload
# we can run our training jobs on A100 80GB or H100 80GB, prefer the lowest cost for this compute class
# so spot nodes first, then DWS
apiVersion: cloud.google.com/v1
kind: ComputeClass
metadata:
  name: gemma3-inf-nodes-l4
spec:
  nodePoolAutoCreation:
    enabled: true
  autoscalingPolicy:
    consolidationDelayMinutes: 2    # wait two minutes before we clean underutilized nodes
  nodePoolConfig:
    serviceAccount: kueue-dev-sa@jkwng-kueue-dev.iam.gserviceaccount.com
  priorities:
  - machineFamily: g2
    gpu:
      count: 1
      type: nvidia-l4
    spot: true
    storage:
      bootDiskSize: 100
      bootDiskType: pd-balanced
      secondaryBootDisks:
      - diskImageName: packer-1752671560
 
  - machineFamily: g2
    gpu:
      count: 1
      type: nvidia-l4
    storage:
      bootDiskSize: 100
      bootDiskType: pd-balanced
      secondaryBootDisks:
      - diskImageName: packer-1752671560
    
  whenUnsatisfiable: DoNotScaleUp   # if pod cannot be satisfied by any of the above, leave the pod pending
